{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Learning from Data\n",
    "\n",
    "In this exercise, you will train different types of regression and classification models on two datasets. You will be graded based on a combination of your code producing the expected results, your written responses to the questions and a passing leaderboard score in the final exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear / Logistic Regression for digit recognition\n",
    "\n",
    "In this part you will experiment with two different types of prediction models, and qualitatively + quantitatively compare them. You will be working with the classic MNIST dataset, which we can load from `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the first few digits and their labels. Apparently they are in order, but our upcoming models will not rely on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=12, figsize=(15, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Warmup Exercise\n",
    "\n",
    "Use the `train_test_split` function from `sklearn.model_selection` twice to split the data into train, validation, and test sets, with a distribution of 80%, 10%, and 10% respectively. Use `random_state=0` for reproducibility.\n",
    "Check out the documentation to figure out how to set the parameters to get the desired split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE. ASSIGN TO THE FOLLOWING VARIABLES, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# >\n",
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, X_temp, y_train, y_temp) = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0, shuffle = True)\n",
    "(X_val, X_test, y_val, y_test) = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out the returned training data, which is of the type `numpy.ndarray`, which you might have seen already. We want to make sure the classes are balanced, e.g. we don't have a lot more 4's than 5's.\n",
    " - How many images do we have for the train, validation, and test sets?\n",
    " - What is the distribution of the training labels? Comment on whether the classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images:\n",
      "X_train : 1437\n",
      "X_test : 180\n",
      "X_val : 180\n",
      " \n",
      "distribution\n",
      "X_train : 80%\n",
      "X_test : 10%\n",
      "X_val : 10%\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE AND WRITE YOUR ANSWERS HERE\n",
    "# >\n",
    "print('# of images:')\n",
    "print('X_train : ' + str(X_train.shape[0]))\n",
    "print('X_test : ' + str(X_test.shape[0]))\n",
    "print('X_val : ' + str(X_val.shape[0]))\n",
    "\n",
    "print(\" \")\n",
    "print(\"distribution\")\n",
    "print('X_train : ' + str(round(((X_train.shape)[0] / digits.data.shape[0] * 100))) + '%')\n",
    "print('X_test : ' + str(round(((X_test.shape)[0] / digits.data.shape[0] * 100))) + '%')\n",
    "print('X_val : ' + str(round(((X_val.shape)[0] / digits.data.shape[0] * 100))) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Programming Exercise\n",
    "\n",
    "Let's build a model that learns to predict the digit given the image. We'll first try our hand at linear regression. We will use the `LinearRegression` class from `sklearn.linear_model`. Again, check out the documentation on how to use it. We will use the `fit` method to train the model, and the `predict` method to make predictions on the test set.\n",
    "Afterwards, evaluate the model using functions from `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MSE:  3.088888888888889\n",
      "LinearRegression accuracy:  0.22777777777777777\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEFINE A LinearRegression MODEL USING SKLEARN. ASSIGN THE MODEL TO THE VARIABLE model.\n",
    "# >\n",
    "import numpy as np\n",
    "import sklearn.linear_model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# FIT THE MODEL AND MAKE PREDICTIONS ON THE TEST SET. ASSIGN THE PREDICTIONS TO THE VARIABLE y_pred.\n",
    "# >\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = ([round(x) for x in model.predict(X_test)])\n",
    "\n",
    "\n",
    "# ASSIGN YOUR MODEL'S TEST SET MSE TO THE VARIABLE mse AND ITS ACCURACY TO THE VARIABLE acc.\n",
    "# >\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# DO NOT INSERT OR CHANGE ANYTHING BELOW\n",
    "print(\"LinearRegression MSE: \", mse)\n",
    "print(\"LinearRegression accuracy: \", acc)\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, y_pred):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: Programming Exercise\n",
    "\n",
    "Now, let's model the task differently.\n",
    "You will use the `LogisticRegression`, again included in the module `sklearn.linear_model`, and again, check out the documentation to see how to use it. Fit the model and make predictions on the test set. \n",
    "\n",
    "You are asked to produce output with the same structure as in the above programming exercise (two printouts and one image), but for a different model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1388888888888888\n",
      "0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# >\n",
    "modelR = sklearn.linear_model.LogisticRegression(max_iter=2170)\n",
    "modelR.fit(X_train, y_train)\n",
    "y_predR = ([x for x in modelR.predict(X_test)])\n",
    "\n",
    "\n",
    "mseR = mean_squared_error(y_test, y_predR)\n",
    "accR = accuracy_score(y_test, y_predR)\n",
    "print(mseR)\n",
    "print(accR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) Describe the trends you observe in the results. Is there anything interesting or unexpected? <br>\n",
    "    - From these result we see that: the lower the MSE the higer the accuracy, and vice versa \n",
    "\n",
    "1b) How do the results compare to the linear regression model? <br>\n",
    "    - The results of logistic regression were remarkably better than the linear regression model.\n",
    "\n",
    "1c) What do you think is the reason for this?<br>\n",
    "    - Since we're trying to classify diffrent numbers logistic regression worked best, if we were to predict a numeric value based on trends Linear regression would work best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Counting stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your team is tasked with applying data science and machine learning techniques to some recently collected telescope image data. The telescope takes low resolution images, in which stars either appear as squares or circles. Your task is to train a classifier to determine whether there are more circular stars than square stars in each image. The budget is tight this year so there are no resources available to train expensive deep neural networks; you will need to train a linear or logistic regression classifier instead.\n",
    "\n",
    "You are encouraged to be creative, and to try out different things empirically, too. Preprocessing could be crucial to the success of your model. Good luck!\n",
    "\n",
    "Below is a sample of the data you'll be working with. Above each image is the label, which shows the number of circular and square stars, respectively. We do not count the small noisy background stars. \n",
    "\n",
    "![](sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D: Programming Exercise\n",
    "\n",
    "1. Create an account on Kaggle to collect the data https://www.kaggle.com/competitions/ds2023-counting-stars/. Use your real name, or tell us your Kaggle username, so we can grade you.\n",
    "2. Download the training data from https://www.kaggle.com/competitions/ds2023-counting-stars/data. Check out the Dataset Description for more information about the data and your task.\n",
    "2. Load the training data file `stars_train_X_y.npz` and split it into `train`, `val`, and `test` splits.\n",
    "3. Use a linear or logistic regression model to fit the `train` data. Use the `val` and `test` splits for model selection and hyperparameter optimization. (You should measure performance using accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##### -- Load data -- #####\n",
    "data = np.load('stars_train_X_y.npz')\n",
    "X = data['X_train'] #   np.ndarray of size (8000, 4096),  flattened image (64x64) of the night sky.\n",
    "y = data['y_train'] #   np.ndarray of size (8000, 2), number of circular stars and number square stars\n",
    "\n",
    "###### -- Preprocessing -- #####\n",
    "''' Convert to binary list\n",
    "    1 if there is more circular stars, 0 otherwise '''\n",
    "def convertBinary(input):\n",
    "    lst = []\n",
    "    for i in range (0, len(input)):\n",
    "        if (input[i][0]) > (input[i][1]):\n",
    "            lst.append(1)\n",
    "        else: lst.append(0)\n",
    "    return lst\n",
    "y = convertBinary(y)\n",
    "\n",
    "\n",
    "'''Removes noise'''\n",
    "def lstPop(inp):\n",
    "    lst = inp\n",
    "    for i in range(len(lst)):\n",
    "        if i == 0 or i == len(lst)-1:  # check if first or last element\n",
    "            continue\n",
    "        if lst[i-1] == 0 and lst[i+1] == 0 and lst[i] != 0:\n",
    "            lst[i] = 0\n",
    "    return lst\n",
    "\n",
    "''' Reverse dimentionality reduction'''\n",
    "def divide(input):\n",
    "    lst = []\n",
    "    for i in range (0, len(input)):\n",
    "        arr1 = []\n",
    "        arr2 = []\n",
    "        for j in range (0, len(input[i])):\n",
    "            if input[i][j] == 0 or input[i][j] == 2:\n",
    "                arr2.append(0)\n",
    "            else:\n",
    "                arr2.append(1)\n",
    "            if input[i][j] == 0 or input[i][j] == 1:\n",
    "                arr1.append(0)\n",
    "            else:\n",
    "                arr1.append(2)\n",
    "        arr1 = lstPop(arr1)\n",
    "        arr2 = lstPop(arr2)\n",
    "        lst.append((arr2)+(arr1))\n",
    "    return lst\n",
    "X = (divide(X))\n",
    "\n",
    "##### -- Split data -- #####\n",
    "(X_train, X_temp, y_train, y_temp) = train_test_split(X, y, test_size=0.2, random_state=0, shuffle = True)\n",
    "(X_test, X_val, y_test, y_val) = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def gridSearch():\n",
    "    hyperparameters = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'class_weight': ['balanced', None],\n",
    "    }\n",
    "    model = sklearn.linear_model.LogisticRegression()\n",
    "    grid_search = GridSearchCV(model, hyperparameters, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "    print(\"Test score: \", grid_search.score(X_val, y_val))\n",
    "gridSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 0.045\n",
      "ACC : 0.955\n"
     ]
    }
   ],
   "source": [
    "##### -- Prediction -- #####\n",
    "def predict(trainX_, trainY_, testX_, testY_ ):\n",
    "    model = sklearn.linear_model.LogisticRegression(C = 0.01, penalty = 'l2', solver = 'liblinear', fit_intercept=True, random_state=0, max_iter=1500)\n",
    "    model.fit(trainX_, trainY_)\n",
    "    pred = model.predict(testX_)\n",
    "    MSE = mean_squared_error(testY_, pred)\n",
    "    ACC = accuracy_score(testY_, pred)\n",
    "    print(\"MSE : \" + str(MSE))\n",
    "    print(\"ACC : \" + str(ACC) )\n",
    "    return pred\n",
    "\n",
    "prediction = predict(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def predictNonLabeled(trainX_, trainY_, testX_):\n",
    "    model = sklearn.linear_model.LogisticRegression(C = 0.01, penalty = 'l2', solver = 'liblinear', fit_intercept=True, random_state=0, max_iter=1500)\n",
    "    model.fit(trainX_, trainY_)\n",
    "    pred = model.predict(testX_)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E: Unlabelled data evaluation\n",
    "\n",
    "You should download the unlabeled test data from https://www.kaggle.com/competitions/ds2023-counting-stars/data and make predictions for all of the unlabeled images in the test set. You can submit your best predictions to Kaggle three times/day.\n",
    "\n",
    "The course responsible has submitted a naive baseline and we're sure you can beat that score! This programming exercise is considered passed if you achieve  $\\text{Score} \\ge 0.90$ on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE TEST SET AND MAKE PREDICTIONS. ASSIGN THE PREDICTIONS TO THE VARIABLE y_pred.\n",
    "# >\n",
    "data = np.load('stars_test_X.npz')\n",
    "testSet = data['X_test']\n",
    "testSet = divide(testSet)\n",
    "y_pred = predictNonLabeled(X_train, y_train, testSet)\n",
    "\n",
    "\n",
    "output_file = \"prediction.csv\"\n",
    "# DO NOT INSERT OR CHANGE ANYTHING BELOW\n",
    "# Use this function to save your predictions on stars_test_X.npz to a \"prediction.csv\" file.\n",
    "# The produced csv file can be directly uploaded to Kaggle for evaluation.\n",
    "def y_pred_to_kaggle_format (y_pred : np.ndarray) -> None:\n",
    "    np.c_[np.arange(1, len(y_pred)+1), y_pred]\n",
    "    np.savetxt(output_file, np.c_[np.arange(1, len(y_pred)+1), y_pred], fmt='%i', delimiter=\",\", header=\"Id,Category\", comments='')\n",
    "\n",
    "y_pred_to_kaggle_format(y_pred)\n",
    "\n",
    "# NOW YOU CAN UPLOAD THE prediction.csv FILE TO KAGGLE AND SEE YOUR ACCURACY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Questions\n",
    "\n",
    "<h3> 2a) Explain the different techniques and their purpose in your code for Programming Exercise D. </h3>\n",
    "<b>Preprocessing of X. </b> <br>\n",
    "For every element in X, I did the following:\n",
    "<ul>\n",
    "<li>Split the dataset into 2 64X64 arrays where in the 1st array, all the 2's were converted to 0, and in the second array, all the 1's were converted into 0's. </li>\n",
    "<li>I removed \"lonely pixels\" for both arrays to reduce noise. This resulted in slightly better accuracy.</li>\n",
    "<li>Then I concatenated the two arrays. The process of splitting and concatenation made a significant impact on the models' accuracy. </li>\n",
    "</ul> <br>\n",
    "<b>Preprocessing of y: </b> <br>\n",
    "Since I'm dealing with binary classification, I had to convert y into a binary list. Each element is equivalent to a 1 if there are more circular stars and a 0 if there are more square stars. <br> <br>\n",
    "\n",
    "<b>Grid search</b> <br>\n",
    "I used Gridsearch for hyperparameter turning.\n",
    "\n",
    "\n",
    "<h3>2b) Which techniques were not useful in improving performance on this task? </h3>\n",
    "Before using grid search, I tuned the hyper parameters by try and error. A lot of the parameters either didn't improve or decreased the accuracy.\n",
    "\n",
    "<b>Linear regression</b> <br>\n",
    "The accuracy decreased significantly when I used linear regression, but I should have suspected this because I was using it for binary classification. If I instead had used linear regression to predict the number of circular and square stars and compared the result, linear regression properly would be sufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
